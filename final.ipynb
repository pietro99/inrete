{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dati/dataset_finalissimo.parquet')\n",
    "materiale_columns = [\"('MATERIALE_1', 'sum')\", \"('MATERIALE_2', 'sum')\", \"('MATERIALE_3', 'sum')\", \"('MATERIALE_4', 'sum')\"]\n",
    "df[\"MATERIALE_TOTAL\"] = df[materiale_columns].sum(axis=1)\n",
    "for col in materiale_columns:\n",
    "    df[col] = df[col] / df['MATERIALE_TOTAL'] \n",
    "df = df.drop(columns=['MATERIALE_TOTAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df[\"('ANNO_POSA', 'median')\"].min()\n",
    "df[\"('ANNO_POSA', 'median')\"] =  df[\"('ANNO_POSA', 'median')\"] - min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_columns = [\"('TIPO_23', 'sum')\", \"('TIPO_24', 'sum')\", \"('TIPO_25', 'sum')\"]\n",
    "df[\"TIPO_TOTAL\"] = df[tipo_columns].sum(axis=1)\n",
    "for col in tipo_columns:\n",
    "    df[col] = df[col] / df['TIPO_TOTAL'] \n",
    "df = df.drop(columns=['TIPO_TOTAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(df: pd.DataFrame):\n",
    "\n",
    "    # trasforma il materiale in percentuali da 0 a 1\n",
    "    materiale_columns = [\"('MATERIALE_1', 'sum')\", \"('MATERIALE_2', 'sum')\", \"('MATERIALE_3', 'sum')\", \"('MATERIALE_4', 'sum')\"]\n",
    "    df[\"MATERIALE_TOTAL\"] = df[materiale_columns].sum(axis=1)\n",
    "    for col in materiale_columns:\n",
    "        df[col] = df[col] / df['MATERIALE_TOTAL'] \n",
    "    df = df.drop(columns=['MATERIALE_TOTAL'])\n",
    "\n",
    "    # stessa cosa per il tipo\n",
    "    tipo_columns = [\"('TIPO_23', 'sum')\", \"('TIPO_24', 'sum')\", \"('TIPO_25', 'sum')\"]\n",
    "    df[\"TIPO_TOTAL\"] = df[tipo_columns].sum(axis=1)\n",
    "    for col in tipo_columns:\n",
    "        df[col] = df[col] / df['TIPO_TOTAL'] \n",
    "    df = df.drop(columns=['TIPO_TOTAL'])\n",
    "\n",
    "    #trasforma anno prosa ad una distanza dall anno piu vecchio (1970)\n",
    "    min = df[\"('ANNO_POSA', 'median')\"].min()\n",
    "    df[\"('ANNO_POSA', 'median')\"] =  df[\"('ANNO_POSA', 'median')\"] - min\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_INDECES = [3, 4, 5,6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "Y_INDECES = [10]\n",
    "len(torch.Tensor(df.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "class InReteDataset(Dataset):\n",
    "    X_INDECES = [3, 4, 5,6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    Y_INDECES = [10]\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.data = torch.Tensor(df.to_numpy())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx, X_INDECES]\n",
    "        label = self.data[idx, Y_INDECES]\n",
    "    \n",
    "        return self.data[idx, X_INDECES+Y_INDECES]\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan import GAN\n",
    "\n",
    "noise_dim = 100  # Dimensionality of the noise vector\n",
    "data_dim = 16  # Output/input dimensionality (e.g., 28x28 image flattened)\n",
    "hidden_dim = 128\n",
    "\n",
    "# Create GAN instance\n",
    "gan = GAN(noise_dim, data_dim, hidden_dim)\n",
    "\n",
    "# Generate a batch of noise vectors\n",
    "batch_size = 12\n",
    "noise = torch.randn(batch_size, noise_dim)\n",
    "\n",
    "# Forward pass through the GAN\n",
    "classification = gan(noise)\n",
    "#print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "df = pd.read_parquet('dati/dataset_finalissimo.parquet')\n",
    "\n",
    "dataset = InReteDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss D: 95.3520, Loss G: 1.4121\n",
      "Epoch [2/100], Loss D: 0.2198, Loss G: 1.6443\n",
      "Epoch [3/100], Loss D: 0.1888, Loss G: 1.7787\n",
      "Epoch [4/100], Loss D: 0.1608, Loss G: 1.9368\n",
      "Epoch [5/100], Loss D: 0.1325, Loss G: 2.1182\n",
      "Epoch [6/100], Loss D: 0.1084, Loss G: 2.3229\n",
      "Epoch [7/100], Loss D: 0.0872, Loss G: 2.5284\n",
      "Epoch [8/100], Loss D: 0.0648, Loss G: 2.8195\n",
      "Epoch [9/100], Loss D: 0.0552, Loss G: 3.0318\n",
      "Epoch [10/100], Loss D: 0.0441, Loss G: 3.2738\n",
      "Epoch [11/100], Loss D: 0.0342, Loss G: 3.5452\n",
      "Epoch [12/100], Loss D: 0.0294, Loss G: 3.7766\n",
      "Epoch [13/100], Loss D: 0.0281, Loss G: 3.9212\n",
      "Epoch [14/100], Loss D: 0.0218, Loss G: 4.2171\n",
      "Epoch [15/100], Loss D: 0.0161, Loss G: 4.3851\n",
      "Epoch [16/100], Loss D: 0.0139, Loss G: 4.6554\n",
      "Epoch [17/100], Loss D: 0.0093, Loss G: 4.8959\n",
      "Epoch [18/100], Loss D: 0.0153, Loss G: 5.1427\n",
      "Epoch [19/100], Loss D: 0.0071, Loss G: 5.3079\n",
      "Epoch [20/100], Loss D: 0.0063, Loss G: 5.4608\n",
      "Epoch [21/100], Loss D: 0.0051, Loss G: 5.7882\n",
      "Epoch [22/100], Loss D: 0.0065, Loss G: 5.8577\n",
      "Epoch [23/100], Loss D: 0.0051, Loss G: 6.0999\n",
      "Epoch [24/100], Loss D: 0.0028, Loss G: 6.3021\n",
      "Epoch [25/100], Loss D: 0.0019, Loss G: 6.5030\n",
      "Epoch [26/100], Loss D: 0.0024, Loss G: 6.5137\n",
      "Epoch [27/100], Loss D: 0.0017, Loss G: 6.8104\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "num_epochs = 100\n",
    "lr = 0.02\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Optimizers\n",
    "optimizer_g = torch.optim.Adam(gan.generator.parameters(), lr=0.002, weight_decay=0.1)\n",
    "optimizer_d = torch.optim.Adam(gan.discriminator.parameters(), lr=0.00002, weight_decay=0.1)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "criterion_disc = nn.BCELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in dataloader:\n",
    "        #real_data = torch.cat(real_data, dim=1)\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        #generate fake sample\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_data = gan.generator(noise)\n",
    "\n",
    "        #predict the real images\n",
    "        distriminator_real_pred = gan.discriminator(real_data)\n",
    "        #predict the real images\n",
    "        distriminator_fake_pred = gan.discriminator(fake_data)\n",
    "\n",
    "        #print(distriminator_fake_pred)\n",
    "        #calculate loss\n",
    "        #print(distriminator_real_pred.shape)\n",
    "        discriminator_loss_real = criterion_disc(distriminator_real_pred,  torch.ones(batch_size, 1))\n",
    "        discriminator_loss_fake = criterion_disc(distriminator_fake_pred, torch.zeros(batch_size, 1))\n",
    "        discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "        \n",
    "        #update weights discriminator\n",
    "        discriminator_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        fake_data = gan.generator(noise).detach()\n",
    "\n",
    "        #predict labels of generated data\n",
    "        generator_fake_label = gan.discriminator(fake_data)\n",
    "        \n",
    "        #calculate loss of generator\n",
    "        g_loss = criterion_disc(generator_fake_label, torch.ones(real_data.size(0), 1))\n",
    "\n",
    "        \n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss D: {discriminator_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(batch_size, noise_dim)\n",
    "fake_data = gan.generator(noise).detach()\n",
    "fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inrete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
