{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prendo i primi due dataset per ogni anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"dati\"\n",
    "years = [2019, 2020, 2021, 2022]\n",
    "\n",
    "tratte_disp_list = []\n",
    "idsap_rischio_list = []\n",
    "\n",
    "for year in years:\n",
    "    year_path = os.path.join(base_path, str(year))\n",
    "\n",
    "    tratte_file = os.path.join(year_path, f\"tratte_disp_{year}.parquet\")\n",
    "    if os.path.exists(tratte_file):\n",
    "        df_tratte = pd.read_parquet(tratte_file)\n",
    "        tratte_disp_list.append(df_tratte)\n",
    "\n",
    "    rischio_folder = os.path.join(year_path, f\"idsap_rischio_{year}\")\n",
    "    if os.path.exists(rischio_folder):\n",
    "        for file in os.listdir(rischio_folder):\n",
    "            if file.endswith(\".snappy.parquet\") and file.startswith(\"part-\"):\n",
    "                file_path = os.path.join(rischio_folder, file)\n",
    "                df_rischio = pd.read_parquet(file_path)\n",
    "                idsap_rischio_list.append(df_rischio)\n",
    "\n",
    "# Concatenazione dei dataset\n",
    "disp_df = pd.concat(tratte_disp_list, ignore_index=True)\n",
    "risk_df = pd.concat(idsap_rischio_list, ignore_index=True)\n",
    "\n",
    "# Output finali\n",
    "print(\"Tratte Disp Dataset Shape:\", disp_df.shape)\n",
    "print(\"ID SAP Rischio Dataset Shape:\", risk_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Li sistemo secondo il codice gia mandato da Lorenza, metto i nan = 0 , aggiungo la colonna anno per dopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.columns = disp_df.columns.str.lower()\n",
    "disp_df['odl'] = disp_df['odl'].astype(str)\n",
    "disp_df['idsap'] = disp_df['idsap'].astype(str)\n",
    "risk_df['idsap'] = risk_df['idsap'].astype(str)\n",
    "disp_df['data'] = pd.to_datetime(disp_df['data'], errors='coerce')\n",
    "risk_df['data'] = pd.to_datetime(risk_df['data'], errors='coerce')\n",
    "print(\"Numero di ID SAP univoci in disp_df:\", disp_df['idsap'].nunique())\n",
    "print(\"Numero di ID SAP univoci in risk_df:\", risk_df['idsap'].nunique())\n",
    "\n",
    "disp_df['anno'] = disp_df['data'].dt.year\n",
    "disp_df['mese'] = disp_df['data'].dt.month\n",
    "\n",
    "disp_df['data'] = pd.to_datetime({'year': disp_df['anno'], 'month': disp_df['mese'], 'day': 1})\n",
    "disp_df = disp_df.drop(columns=['anno', 'mese'])\n",
    "\n",
    "disp_tot_df = disp_df.groupby(['idsap', 'data'])['odl'].count().reset_index()\n",
    "disp_tot_df.rename(columns={'odl': 'disp_mensili'}, inplace=True)\n",
    "disp_tot_df\n",
    "\n",
    "merged_df = pd.merge(risk_df, disp_tot_df, on=['idsap', 'data'], how='left')\n",
    "merged_df['disp_mensili'] = merged_df['disp_mensili'].fillna(0)\n",
    "merged_df['date'] = merged_df['data'].dt.year.astype('int64')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carica i geometry e aggiungo l anno per mergarli dopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"dati_finali_disp\"\n",
    "\n",
    "# Nomi dei file parquet\n",
    "file_names = [\"df1.parquet\", \"df2.parquet\", \"df3.parquet\", \"df4.parquet\"]\n",
    "\n",
    "# Caricare i file parquet e inserirli in una lista\n",
    "dataframes = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(input_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_parquet(file_path)\n",
    "        dataframes.append(df)\n",
    "        print(f\"Loaded {file_name} with shape {df.shape}\")\n",
    "    else:\n",
    "        print(f\"File {file_name} not found!\")\n",
    "\n",
    "print(f\"Total DataFrames loaded: {len(dataframes)}\")\n",
    "df1=dataframes[0]\n",
    "df2=dataframes[1]\n",
    "df3=dataframes[2]\n",
    "df4=dataframes[3]\n",
    "\n",
    "df1['date'] = 2019\n",
    "df2['date'] = 2020\n",
    "df3['date'] = 2021\n",
    "df4['date'] = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNISCO I DF GEOMETRY ASSIEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dataframes(*dataframes):\n",
    "    \"\"\"\n",
    "    Concatena i DataFrame forniti in ordine, mettendo le righe di ciascun DataFrame\n",
    "    dopo quelle del precedente.\n",
    "    \n",
    "    Args:\n",
    "    - *dataframes (pd.DataFrame): Sequenza di DataFrame da concatenare.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame risultante dalla concatenazione.\n",
    "    \"\"\"\n",
    "    result = pd.concat(dataframes, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "merged_df_geometry = concatenate_dataframes(df1, df2, df3, df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORA ABBIAMO TUTTO\n",
    "**merged_df**   \n",
    "**merged_df_geometry le geometry per anno per ogni idsap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={\"idsap\": \"IDSAP\"}, inplace=True)\n",
    "\n",
    "\n",
    "def colonna_unica(df,col_name):\n",
    "    unique_elements = df[col_name].value_counts()\n",
    "    print(\"LEN(DF):\", len(df))\n",
    "    print(\"LEN(unique,idsap):\", len(unique_elements))\n",
    "    if((len(unique_elements))<100):\n",
    "        # Stampare i risultati\n",
    "        print(f\"Elementi unici nella colonna {col_name}:\")\n",
    "        for elem, count in unique_elements.items():\n",
    "            print(f\": {elem}, Count: {count}\")\n",
    "\n",
    "idsap_unici(merged_df,\"IDSAP\")\n",
    "idsap_unici(merged_df_geometry,\"IDSAP\")\n",
    "\n",
    "idsap_unici(df1,\"IDSAP\")\n",
    "idsap_unici(df2,\"IDSAP\")\n",
    "idsap_unici(df3,\"IDSAP\")\n",
    "idsap_unici(df4,\"IDSAP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRENDI L INTERSEZIONE TRA DF_MERGED E GLI ALTRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_intersection(df1, df2, column_name=\"IDSAP\"):\n",
    "    \"\"\"\n",
    "    Confronta gli elementi di una colonna tra due DataFrame e lascia solo l'intersezione.\n",
    "    \n",
    "    Args:\n",
    "    - df1 (pd.DataFrame): Primo DataFrame.\n",
    "    - df2 (pd.DataFrame): Secondo DataFrame.\n",
    "    - column_name (str): Nome della colonna su cui basare l'intersezione.\n",
    "    \n",
    "    Returns:\n",
    "    - (pd.DataFrame, pd.DataFrame): I due DataFrame filtrati.\n",
    "    \"\"\"\n",
    "    print(f\"Length of DataFrame 1 before filtering: {len(df1)}\")\n",
    "    print(f\"Length of DataFrame 2 before filtering: {len(df2)}\")\n",
    "    \n",
    "    # Trova l'intersezione degli elementi nella colonna specificata\n",
    "    common_elements = set(df1[column_name]).intersection(set(df2[column_name]))\n",
    "    \n",
    "    # Filtra entrambi i DataFrame mantenendo solo i valori comuni\n",
    "    df1_filtered = df1[df1[column_name].isin(common_elements)].reset_index(drop=True)\n",
    "    df2_filtered = df2[df2[column_name].isin(common_elements)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Length of DataFrame 1 after filtering: {len(df1_filtered)}\")\n",
    "    print(f\"Length of DataFrame 2 after filtering: {len(df2_filtered)}\")\n",
    "    \n",
    "    return df1_filtered, df2_filtered\n",
    "\n",
    "\n",
    "merged_df_filtered,merged_df_geometry_filtered = filter_by_intersection(merged_df, merged_df_geometry, column_name=\"IDSAP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_geometry_filtered.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREO DATASET FINALE GIGANTE CON TUTTO MERGANDO LE GEOMETRY E LE TRATTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_on_idsap_and_date(df1, df2):\n",
    "    \"\"\"\n",
    "    Esegue un merge tra due DataFrame sulla base delle colonne 'IDSAP' e 'date'.\n",
    "    \n",
    "    Args:\n",
    "    - df1 (pd.DataFrame): Il primo DataFrame.\n",
    "    - df2 (pd.DataFrame): Il secondo DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Il DataFrame risultante dal merge.\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(df1, df2, on=['IDSAP', 'date'], how='inner')\n",
    "    return merged_df\n",
    "\n",
    "# Esempio di utilizzo\n",
    "final_df = merge_on_idsap_and_date(merged_df_filtered, merged_df_geometry_filtered)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(f\"Shape of df1: {merged_df_filtered.shape}\")\n",
    "print(f\"Shape of df2: {merged_df_geometry_filtered.shape}\")\n",
    "print(f\"Shape of merged_df: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sorted_idsap_date = final_df.sort_values(by=['IDSAP', 'date'])\n",
    "final_df_sorted_idsap_date.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset, un po inutile ci mette piu a fare l upload che a crearlo :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"final_df_sorted_idsap_date.parquet\"\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "final_df_sorted_idsap_date.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifica il percorso del file Parquet\n",
    "input_path = \"final_df_sorted_idsap_date.parquet\"\n",
    "\n",
    "# Carica il DataFrame da Parquet\n",
    "final_df_sorted_idsap_date = pd.read_parquet(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colonna_unica(df,col_name):\n",
    "    unique_elements = df[col_name].value_counts()\n",
    "    print(\"LEN(DF):\", len(df))\n",
    "    print(\"LEN(unique,idsap):\", len(unique_elements))\n",
    "    if((len(unique_elements))<100):\n",
    "        # Stampare i risultati\n",
    "        print(f\"Elementi unici nella colonna {col_name}:\")\n",
    "        for elem, count in unique_elements.items():\n",
    "            print(f\": {elem}, Count: {count}\")\n",
    "\n",
    "colonna_unica(final_df_sorted_idsap_date,\"CODSISTEMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGGIUNGO ANNO E MESE AL DATASET FINALE PER POTER GRUPPARE DOPO PER IMPIANTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Index(['data', 'IDSAP', 'risk_level', 'disp_mensili', 'date', 'TIPO',\n",
    "    'MATERIALE', 'DIAMETRO', 'ANNO_POSA', 'CODSISTEMA', 'lenght',\n",
    "    'is_closed', 'is_simple', 'is_ring', 'minimum_clearance', 'centroid_x',\n",
    "    'centroid_y', 'bound_0', 'bound_1', 'bound_2', 'bound_3',\n",
    "    'x_coordinates', 'y_coordinates'],\n",
    "    dtype='object')\n",
    "\"\"\"\n",
    "final_df_sorted_idsap_date['year'] = final_df_sorted_idsap_date['data'].dt.year\n",
    "final_df_sorted_idsap_date['month'] = final_df_sorted_idsap_date['data'].dt.month\n",
    "\n",
    "final_df_sorted_idsap_date.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL_DF HA QUELLI PRESENTI 48 VOLTE , DF_GROUPED ANCHE QUELLI CON QUALCHE NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleziona solo le colonne numeriche\n",
    "numeric_columns = final_df_sorted_idsap_date.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Esegui il groupby per 'CODSISTEMA', 'year', 'month' e calcola la media per tutte le colonne numeriche\n",
    "df_grouped = final_df_sorted_idsap_date.groupby(['CODSISTEMA', 'year', 'month'], as_index=False)[numeric_columns].mean()\n",
    "\n",
    "counts_per_codsistema = df_grouped.groupby('CODSISTEMA').size()\n",
    "# Filtra i CODSISTEMA che hanno esattamente 48 righe\n",
    "valid_codsistema = counts_per_codsistema[counts_per_codsistema == 48].index\n",
    "\n",
    "# Filtra il DataFrame finale per mantenere solo i CODSISTEMA con 48 righe\n",
    "final_df_filtered = df_grouped[df_grouped['CODSISTEMA'].isin(valid_codsistema)]\n",
    "\n",
    "# Verifica la dimensione del nuovo DataFrame\n",
    "\n",
    "print(final_df_filtered.shape)\n",
    "print(df_grouped.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inrete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
